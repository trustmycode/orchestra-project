---
id: TASK-2024-033
title: "Эпик: Реализация `ai-service` с локальной LLM и структурированным выводом"
status: backlog
priority: medium
type: feature
estimate: 22h
created: 2024-07-30
updated: 2024-07-30
parents: [TASK-2024-015]
children: [TASK-2024-034, TASK-2024-035, TASK-2024-036, TASK-2024-037, TASK-2024-038]
arch_refs: [ARCH-core-services, ADR-0016, ADR-0017]
audit_log:
  - {date: 2024-07-30, user: "@RoboticArchitect", action: "created with status backlog"}
---
## Описание
Этот эпик объединяет задачи по созданию и интеграции полноценного `ai-service`. Сервис будет использовать локально развернутую LLM (через Ollama) и фреймворк Spring AI для генерации надежных, структурированных тестовых данных (JSON).
Эта работа заменяет и детализирует первоначальную задачу TASK-2024-017.

## Критерии приемки
- Все дочерние задачи выполнены.
- Система способна генерировать тестовые данные для HTTP-шагов с использованием локальной AI-модели.
- Процесс полностью автоматизирован и интегрирован в локальное dev-окружение Docker Compose.
